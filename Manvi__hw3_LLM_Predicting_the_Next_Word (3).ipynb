{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpphM51oUqQn"
   },
   "source": [
    "## HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dxFPIroyVCH8"
   },
   "outputs": [],
   "source": [
    "# Run this line if you do not have this package in your environment\n",
    "!pip install portalocker>=2.0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Uw5ffQMUqQq"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mSTtyPlQUqQr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OWYomTdNUqQt"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Build the vocabulary (dictionary of words) from text dataset\n",
    "def build_vocab(data_iter):\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, data_iter), specials=[\"<pad>\", \"<unk>\", \"<SOS>\", \"<EOS>\"])\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    return vocab\n",
    "\n",
    "train_data, _, _ = torchtext.datasets.PennTreebank(root='data', split=('train', 'valid', 'test'))\n",
    "vocab = build_vocab(train_data)\n",
    "\n",
    "# Process raw text data. Convert each sentence into a tensor of integers\n",
    "def process_raw_text(raw_text_iter, vocab):\n",
    "    sos_index = vocab['<SOS>']\n",
    "    eos_index = vocab['<EOS>']\n",
    "    # Adding <SOS> at the start and <EOS> at the end of each sequence\n",
    "    data = [torch.tensor([sos_index] + vocab(tokenizer(item)) + [eos_index], dtype=torch.long) for item in raw_text_iter]\n",
    "    return data\n",
    "\n",
    "train_data, valid_data, test_data = torchtext.datasets.PennTreebank(root='data', split=('train', 'valid', 'test'))\n",
    "\n",
    "train_text = process_raw_text(train_data,vocab)\n",
    "valid_text = process_raw_text(valid_data,vocab)\n",
    "test_text = process_raw_text(test_data,vocab)\n",
    "\n",
    "# limit the train text data to 20000 sentences\n",
    "train_text = train_text[:20000]\n",
    "\n",
    "#this code prepares text data for training an NLP model by tokenizing, building a vocabulary, and converting text into tensors.\n",
    "#The Penn Treebank dataset is commonly used for language modeling tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_Zjj3NFUqQu",
    "outputId": "4914cd11-92f7-43b1-fc21-259e3004134d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', '<SOS>', '<EOS>', 'the', 'n', 'of']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can look up the representation of each token using lookup_tokens()\n",
    "vocab.lookup_tokens([0, 1, 2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtVQ3xfCvNoB",
    "outputId": "09ad9a05-210d-48ce-d8c3-2c4291336aff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no it was n't black monday\n",
      "\n",
      "\n",
      "but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos\n",
      "\n",
      "\n",
      "some circuit breakers installed after the october N crash failed their first test traders say unable to cool the selling panic in both stocks and futures\n",
      "\n",
      "\n",
      "the N stock specialist firms on the big board floor the buyers and sellers of last resort who were criticized after the N crash once again could n't handle the selling pressure\n",
      "\n",
      "\n",
      "big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock traders say\n",
      "\n",
      "\n",
      "heavy selling of standard & poor 's 500-stock index futures in chicago <unk> beat stocks downward\n",
      "\n",
      "\n",
      "seven big board stocks ual amr bankamerica walt disney capital cities\\/abc philip morris and pacific telesis group stopped trading and never resumed\n",
      "\n",
      "\n",
      "the <unk> has already begun\n",
      "\n",
      "\n",
      "the equity market was <unk>\n",
      "\n",
      "\n",
      "once again the specialists were not able to handle the imbalances on the floor of the new york stock exchange said christopher <unk> senior vice president at <unk> securities corp\n",
      "\n",
      "\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it is easy to say the specialist is n't doing his job\n",
      "\n",
      "\n",
      "when the dollar is in a <unk> even central banks ca n't stop it\n",
      "\n",
      "\n",
      "speculators are calling for a degree of liquidity that is not there in the market\n",
      "\n",
      "\n",
      "many money managers and some traders had already left their offices early friday afternoon on a warm autumn day because the stock market was so quiet\n",
      "\n",
      "\n",
      "then in a <unk> plunge the dow jones industrials in barely an hour surrendered about a third of their gains this year <unk> up a 190.58-point or N N loss on the day in <unk> trading volume\n",
      "\n",
      "\n",
      "<unk> trading accelerated to N million shares a record for the big board\n",
      "\n",
      "\n",
      "at the end of the day N million shares were traded\n",
      "\n",
      "\n",
      "the dow jones industrials closed at N\n",
      "\n",
      "\n",
      "the dow 's decline was second in point terms only to the <unk> black monday crash that occurred oct. N N\n",
      "\n",
      "\n",
      "in percentage terms however the dow 's dive was the <unk> ever and the sharpest since the market fell N or N N a week after black monday\n",
      "\n",
      "\n",
      "the dow fell N N on black monday\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at test dataset\n",
    "count = 0\n",
    "for text in test_data:\n",
    "  print(text)\n",
    "  print('\\n')\n",
    "  count += 1\n",
    "  if count > 20:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYZztNQXUqQv",
    "outputId": "12b64cde-55ca-4894-835f-7c6473772e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary: 9925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming vocab is your vocabulary object\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Number of words in the vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHkAvDYTZW5t",
    "outputId": "b0292681-51b9-481d-83f8-76c940cf4142"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_length= [len(txt) for txt in train_text]\n",
    "print(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vKbf3PIUqQ0"
   },
   "source": [
    "## Question 1. Prepare data for the RNN/LSTM models (fill in the blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "haiEK99cyAra"
   },
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "train_input = [sequence[:-1] for sequence in train_text] # Input sequence is a sentence without the last word. train_input should be a list of texts from train_text.\n",
    "train_length = [len(sequence) for sequence in train_input] # Length is later used for packing (optional) and calculating accuracy for non-padded sequence\n",
    "train_target = [sequence[1:] for sequence in train_text] # Target is a sentence without the first word. train_target should be a list of texts from train_text\n",
    "\n",
    "# Do the same for the validation and test set\n",
    "\n",
    "valid_input = [sequence[:-1] for sequence in valid_text]\n",
    "valid_target = [sequence[1:] for sequence in valid_text]\n",
    "valid_length = [len(sequence) for sequence in valid_input]\n",
    "\n",
    "test_input = [sequence[:-1] for sequence in test_text]\n",
    "test_target = [sequence[1:] for sequence in test_text]\n",
    "test_length = [len(sequence) for sequence in test_input]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XnbmyZIKyBQ-"
   },
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "# Pad the input and target sequences using pad_sequence(). Please read the documentation of the function carefully before filling below.\n",
    "# We need to pass the batch_first argument\n",
    "train_input_padded = pad_sequence(train_input, batch_first=True).numpy()\n",
    "train_target_padded = pad_sequence(train_target, batch_first=True).numpy()\n",
    "\n",
    "valid_input_padded = pad_sequence(valid_input, batch_first=True).numpy()\n",
    "valid_target_padded = pad_sequence(valid_target, batch_first=True).numpy()\n",
    "\n",
    "test_input_padded = pad_sequence(test_input, batch_first=True).numpy()\n",
    "test_target_padded = pad_sequence(test_target, batch_first=True).numpy()\n",
    "\n",
    "#sequences are being padded to the maximum length found in each respective dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcH11yL3yDzG",
    "outputId": "b88e423e-97ed-45f1-9782-cabc3ec25658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 84)\n",
      "(3370, 77)\n",
      "(3761, 86)\n"
     ]
    }
   ],
   "source": [
    "print(train_input_padded.shape)\n",
    "print(valid_input_padded.shape)\n",
    "print(test_input_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NCUBsjmzyKEU"
   },
   "outputs": [],
   "source": [
    "# Define a dataset and dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.X[idx][0]\n",
    "        length = self.X[idx][1]\n",
    "        return torch.tensor(text,dtype=torch.long), torch.tensor(length,dtype=torch.long), torch.tensor(self.y[idx],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nkf1nez1yKlX"
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(list(zip(train_input_padded, train_length)), train_target_padded)\n",
    "valid_dataset = TextDataset(list(zip(valid_input_padded, valid_length)), valid_target_padded)\n",
    "test_dataset = TextDataset(list(zip(test_input_padded, test_length)), test_target_padded)\n",
    "\n",
    "# Define a dataloader\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETx2Pg8Iwb6t"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhanOB_zUqQz"
   },
   "source": [
    "### Build Naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EOS6ZAMvxjU0"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#function that gets the most frequent word in a text dataset\n",
    "\n",
    "def get_most_frequent_word(text_data):\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for words in text_data:\n",
    "        word_counter.update(words)\n",
    "#most_common(n)returns a list with a tuple with the n most common element and its count.\n",
    "    most_frequent_word = word_counter.most_common(1)[0][0]  # Get the most common word\n",
    "    return most_frequent_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7431Ep-fpP26"
   },
   "outputs": [],
   "source": [
    "def get_text_data(X, y):\n",
    "    text_data = []\n",
    "    for inp, out in zip(X, y):\n",
    "        inp_filtered = inp[inp > 0]  # Assuming zero is used for padding\n",
    "        out_filtered = out[out > 0]\n",
    "        full_sequence = np.concatenate((inp_filtered, out_filtered))\n",
    "        text_data.append(full_sequence)\n",
    "    return text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRGTEl0VRNCY",
    "outputId": "7ce9784a-db50-4d0e-c431-8a9d871d1e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the naive model on the test set: 5.27%\n",
      "Most frequent word predicted: 4\n"
     ]
    }
   ],
   "source": [
    "#calculates accuracy of predicting the most frequent word in a dataset\n",
    "def evaluate_naive_model(text_data, most_frequent_word):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sequence in text_data:\n",
    "        for word in sequence:\n",
    "            if word == most_frequent_word:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = (100 * correct / total) if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Assuming train_input, train_target, test_input, test_target are arrays loaded and prepared\n",
    "train_text = get_text_data(train_input, train_target)\n",
    "test_text = get_text_data(test_input, test_target)\n",
    "\n",
    "# Get the most frequent word from the training text\n",
    "most_frequent_word = get_most_frequent_word(train_text)\n",
    "\n",
    "# Evaluate the model on the test text\n",
    "test_accuracy = evaluate_naive_model(test_text, most_frequent_word)\n",
    "print(f\"Accuracy of the naive model on the test set: {test_accuracy:.2f}%\")\n",
    "print(f\"Most frequent word predicted: {most_frequent_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ty3eurIhqMK5"
   },
   "outputs": [],
   "source": [
    "def get_most_frequent_word(text_data):\n",
    "    \"\"\"Finds the most frequently occurring word in the text data.\"\"\"\n",
    "    word_counter = Counter()\n",
    "    for sequence in text_data:\n",
    "        word_counter.update(sequence)\n",
    "    return word_counter.most_common(1)[0][0]\n",
    "\n",
    "def predict_with_fallback(model, key, fallback):\n",
    "    \"\"\"Predicts using the model or falls back to the most frequent word if the key is not found.\"\"\"\n",
    "# model.get(key, default) is a method to retrieve value associated with a key in a dictionary-like object. It returns the value matching the key if the key is in the dictionary\n",
    "# otherwise,it returns the default value.\n",
    "    return model.get(key, fallback)\n",
    "\n",
    "\n",
    "#fucntion to evaluate the model being used\n",
    "def nw_accuracy(text_data, model, fallback, n_gram=1):\n",
    "    \"\"\"Evaluates the model accuracy, reverting to fallback for unseen tokens.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sequence in text_data:\n",
    "        for i in range(len(sequence) - n_gram):\n",
    "            if n_gram == 1:\n",
    "                key = sequence[i]\n",
    "            else:  # Assuming n_gram == 2 for bigram\n",
    "                key = (sequence[i], sequence[i+1])\n",
    "\n",
    "            if i + n_gram < len(sequence):\n",
    "                actual_next = sequence[i + n_gram]\n",
    "                predicted_next = model.get(key, fallback)\n",
    "                if predicted_next == actual_next:\n",
    "                    correct += 1\n",
    "            total += 1\n",
    "\n",
    "    return 100 * correct / total if total else 0\n",
    "\n",
    "  # Assuming train_input, train_target, test_input, test_target are loaded and prepared\n",
    "train_text = get_text_data(train_input, train_target)\n",
    "test_text = get_text_data(test_input, test_target)\n",
    "#most freq word for baseline fallback\n",
    "most_frequent_word = get_most_frequent_word(train_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAg4nTnUUqQz"
   },
   "source": [
    "### Build one-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08LsKJslsBY2",
    "outputId": "da9b0cbc-2213-45d0-d00f-f7fb63700805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the one-gram model on the test set: 19.32%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_text_data(X, y):\n",
    "    \"\"\"Extracts sequences from the input and target arrays, filtering out padding.\"\"\"\n",
    "    text_data = []\n",
    "    for inp, out in zip(X, y):\n",
    "        inp_filtered = inp[inp > 0]  # Assuming padding is represented by zeros\n",
    "        out_filtered = out[out > 0]\n",
    "        full_sequence = np.concatenate((inp_filtered, out_filtered))\n",
    "        text_data.append(full_sequence)\n",
    "    return text_data\n",
    "\n",
    "def build_onegram_model(text_data):\n",
    "    \"\"\"Builds a onegram model from the text data.\"\"\"\n",
    "    onegram_counts = defaultdict(Counter)\n",
    "    for sequence in text_data:\n",
    "        for i in range(len(sequence) - 1):\n",
    "            current_token = sequence[i]\n",
    "            next_token = sequence[i+1]\n",
    "            onegram_counts[current_token][next_token] += 1\n",
    "    # Finding the most common next token for each token\n",
    "    onegram_most_common = {token: counts.most_common(1)[0][0] for token, counts in onegram_counts.items()}\n",
    "    return onegram_most_common\n",
    "\n",
    "\n",
    "# Build model\n",
    "onegram_model = build_onegram_model(train_text)\n",
    "#find most common word to evaluate\n",
    "most_frequent_word_onegram = get_most_frequent_word(test_text)\n",
    "# Evaluate model\n",
    "onegram_accuracy = nw_accuracy(test_text, onegram_model, most_frequent_word_onegram, n_gram=1)\n",
    "\n",
    "print(f'Accuracy of the one-gram model on the test set: {onegram_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ns2-Xuyr7pV"
   },
   "source": [
    "### Build bi-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOfRKnZUwgwp",
    "outputId": "c7152d89-9b13-49f7-9d83-e871fe17b8fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the bi-gram model on the test set: 19.37%\n"
     ]
    }
   ],
   "source": [
    "#predicts next word based on the two words before it\n",
    "def build_bigram_model(text_data):\n",
    "    \"\"\"Builds a bigram model from the text data.\"\"\"\n",
    "    bigram_counts = defaultdict(Counter)\n",
    "    for sequence in text_data:\n",
    "      #iterates till third last word bc its the last word in the sequence that will have the word, word+1 to predict word+2\n",
    "        for i in range(len(sequence) - 2):\n",
    "            bigram = (sequence[i], sequence[i+1])\n",
    "            next_word = sequence[i+2]\n",
    "            bigram_counts[bigram][next_word] += 1\n",
    "    # Finding the most common next word for each bi-gram\n",
    "    bigram_most_common = {bigram: counts.most_common(1)[0][0] for bigram, counts in bigram_counts.items()}\n",
    "    return bigram_most_common\n",
    "\n",
    "# Build model\n",
    "bigram_model = build_bigram_model(train_text)\n",
    "#find most common word to evaluate\n",
    "most_frequent_word_bigram = get_most_frequent_word(test_text)\n",
    "#Evaluate model accuracy\n",
    "bigram_accuracy = nw_accuracy(test_text, bigram_model, most_frequent_word, n_gram=2)\n",
    "\n",
    "print(f'Accuracy of the bi-gram model on the test set: {bigram_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e55nDLlyUqQ1"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QcfCpVSDUqQ1"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Redefine the training function and accuracy calculator\n",
    "\n",
    "# Function to calculate accuracy for non-padded sequence and calculate loss, perplexity for padded sequence.\n",
    "def calculate_metrics(loader, model,loss_func=nn.CrossEntropyLoss()):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Calculate Accuracy and Loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    perplexity = 0\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for data, l, target in loader:\n",
    "            # check if cuda is available\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            for i in range(len(l)):\n",
    "              output = model(data[i])  # Forward pass\n",
    "              output_no_padding = output[:l[i],:]\n",
    "              target_no_padding = target[i][:l[i]]\n",
    "              _, predicted = torch.max(output_no_padding.data, 1)  # Get the predicted classes\n",
    "              total +=target_no_padding.view(-1).size(0)\n",
    "              correct += (predicted == target_no_padding.view(-1)).sum().item()\n",
    "              # calculate loss (calculated on padded sequence)\n",
    "              loss = loss_func(output.view(-1,9925),target[i].view(-1))\n",
    "              test_loss += loss.item()\n",
    "              # calculate perplexity\n",
    "              perplexity += 2**loss.item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    test_loss /= len(loader.dataset)\n",
    "    perplexity /= len(loader.dataset)\n",
    "    model.train()\n",
    "    # round accuracy, test_loss, perplexity to 2 decimal places\n",
    "    accuracy = round(accuracy,2)\n",
    "    test_loss = round(test_loss,2)\n",
    "    perplexity = round(perplexity,2)\n",
    "    return accuracy, test_loss, perplexity\n",
    "\n",
    "def train_model(model, loss_func, num_epochs, optimizer, train_loader, test_loader):\n",
    "\n",
    "  train_loss_log = []\n",
    "  train_acc_log = []\n",
    "  test_acc_log = []\n",
    "\n",
    "  # Move model to GPU if CUDA is available\n",
    "  if torch.cuda.is_available():\n",
    "      model = model.cuda()\n",
    "  tic = time.time()\n",
    "  for epoch in range(1,num_epochs+1):\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "      x, l, y = data\n",
    "      # check if cuda is available\n",
    "      if torch.cuda.is_available():\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      # get predicted y value from our current model\n",
    "      pred_y = model(x)\n",
    "      # calculate the loss\n",
    "      loss = loss_func(pred_y.view(-1,9925),y.view(-1))\n",
    "      # Zero the gradient of the optimizer\n",
    "      optimizer.zero_grad()\n",
    "      # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "      loss.backward()\n",
    "      # update weights ar\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item() * len(x)\n",
    "    # change the model to evaluation mode to calculate the test loss; We will come back to this later after learning Dropout and Batch Normalization\n",
    "    train_loss/=len(train_loader.dataset)\n",
    "    train_loss_log.append(train_loss)\n",
    "    train_acc, _, _ = calculate_metrics(train_loader, model)\n",
    "    test_acc, _, _ = calculate_metrics(test_loader, model)\n",
    "    train_acc_log.append(train_acc)\n",
    "    test_acc_log.append(test_acc)\n",
    "    print(\"Epoch {:2},  Training Loss: {:9.4f}, Training Accuracy: {:9.2f},  Valid Accuracy: {:7.2f}\".format(epoch, train_loss, train_acc, test_acc))\n",
    "  toc = time.time()\n",
    "  print(\"Elapsed Time : {:7.2f}\".format(toc-tic))\n",
    "  return train_loss_log, train_acc_log, test_acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE84E2EFUqQ1"
   },
   "source": [
    "## Question 3. RNN without hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vU4f9Fgrw74I",
    "outputId": "8ee422c5-dfaa-4958-fdb2-8cd1e2823124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Multinomial Logistic Regression Model:\n",
      "Epoch  1,  Training Loss:    2.3640, Training Accuracy:     14.60,  Valid Accuracy:   14.30\n",
      "Epoch  2,  Training Loss:    1.6440, Training Accuracy:     16.68,  Valid Accuracy:   16.20\n",
      "Epoch  3,  Training Loss:    1.5101, Training Accuracy:     17.83,  Valid Accuracy:   17.07\n",
      "Epoch  4,  Training Loss:    1.4439, Training Accuracy:     18.36,  Valid Accuracy:   17.36\n",
      "Epoch  5,  Training Loss:    1.4022, Training Accuracy:     19.15,  Valid Accuracy:   18.01\n",
      "Epoch  6,  Training Loss:    1.3724, Training Accuracy:     19.61,  Valid Accuracy:   18.31\n",
      "Epoch  7,  Training Loss:    1.3494, Training Accuracy:     19.97,  Valid Accuracy:   18.52\n",
      "Epoch  8,  Training Loss:    1.3308, Training Accuracy:     20.30,  Valid Accuracy:   18.66\n",
      "Epoch  9,  Training Loss:    1.3154, Training Accuracy:     20.51,  Valid Accuracy:   18.83\n",
      "Epoch 10,  Training Loss:    1.3024, Training Accuracy:     20.48,  Valid Accuracy:   18.74\n",
      "Elapsed Time :  154.39\n",
      "\n",
      "Training Single Layer Neural Network Model:\n",
      "Epoch  1,  Training Loss:    1.8880, Training Accuracy:     15.61,  Valid Accuracy:   15.47\n",
      "Epoch  2,  Training Loss:    1.4963, Training Accuracy:     17.51,  Valid Accuracy:   17.02\n",
      "Epoch  3,  Training Loss:    1.4303, Training Accuracy:     18.68,  Valid Accuracy:   17.81\n",
      "Epoch  4,  Training Loss:    1.3850, Training Accuracy:     19.08,  Valid Accuracy:   18.09\n",
      "Epoch  5,  Training Loss:    1.3515, Training Accuracy:     19.96,  Valid Accuracy:   18.76\n",
      "Epoch  6,  Training Loss:    1.3258, Training Accuracy:     20.34,  Valid Accuracy:   18.76\n",
      "Epoch  7,  Training Loss:    1.3054, Training Accuracy:     20.61,  Valid Accuracy:   18.96\n",
      "Epoch  8,  Training Loss:    1.2892, Training Accuracy:     20.80,  Valid Accuracy:   19.13\n",
      "Epoch  9,  Training Loss:    1.2760, Training Accuracy:     20.96,  Valid Accuracy:   18.97\n",
      "Epoch 10,  Training Loss:    1.2649, Training Accuracy:     21.18,  Valid Accuracy:   19.06\n",
      "Elapsed Time :  168.24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultinomialLogisticRegression(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(MultinomialLogisticRegression, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: LongTensor of size (batch_size, sequence_length)\n",
    "        embedded = self.embedding(x)\n",
    "        logits = self.linear(embedded)\n",
    "        return logits\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: LongTensor of size (batch_size, sequence_length)\n",
    "        embedded = self.embedding(x)\n",
    "        hidden = self.relu(self.linear1(embedded))\n",
    "        logits = self.linear2(hidden)\n",
    "        return logits\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 9925  # from your dataset info\n",
    "embedding_dim = 100  # size of the embedding vectors\n",
    "hidden_dim = 128  # size of the hidden layer for the single layer NN\n",
    "\n",
    "# Initialize models\n",
    "model_a = MultinomialLogisticRegression(vocab_size, embedding_dim)\n",
    "model_b = SingleLayerNN(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer_a = optim.Adam(model_a.parameters(), lr=0.001)\n",
    "optimizer_b = optim.Adam(model_b.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming `trainloader` and `testloader` are defined as per your data preparation code\n",
    "num_epochs = 10  # or however many you deem appropriate\n",
    "\n",
    "# Train model a\n",
    "print(\"Training Multinomial Logistic Regression Model:\")\n",
    "train_loss_log_a, train_acc_log_a, test_acc_log_a = train_model(model_a, loss_func, num_epochs, optimizer_a, trainloader, validloader)\n",
    "\n",
    "# Train model b\n",
    "print(\"\\nTraining Single Layer Neural Network Model:\")\n",
    "train_loss_log_b, train_acc_log_b, test_acc_log_b = train_model(model_b, loss_func, num_epochs, optimizer_b, trainloader, validloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gi_D2P9cD36S",
    "outputId": "e1bce417-6e35-4395-de9f-f3eb79691dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.06, 1.43, 3.06)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(testloader, model_a,loss_func=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xii5XasQD8LM",
    "outputId": "39f20d8b-991f-450e-80c2-06fc0f214eec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.31, 1.49, 3.26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(testloader, model_b,loss_func=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xeSmiuEyfdt"
   },
   "source": [
    "## Question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfvlT32KUqQ2"
   },
   "source": [
    "### RNN with single hidden state layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1uGKjsuLw7cW"
   },
   "outputs": [],
   "source": [
    "class NextWordRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1): #single hidden layer\n",
    "        super(NextWordRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #embeddings convert discrete word indices into dense vector reps (embeddings) that capture semantic information\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim,num_layers=num_layers, nonlinearity='relu', batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        all_h, last_h = self.rnn(x)\n",
    "        out = self.fc(all_h) # Apply Linear layer to outputs from all the hidden state.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bYIAsmkwu8V",
    "outputId": "b5f8982c-e241-4fb4-d414-662ec23f18a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1,  Training Loss:    1.7349, Training Accuracy:     18.20,  Valid Accuracy:   18.29\n",
      "Epoch  2,  Training Loss:    1.4606, Training Accuracy:     20.86,  Valid Accuracy:   20.61\n",
      "Epoch  3,  Training Loss:    1.3752, Training Accuracy:     22.32,  Valid Accuracy:   21.52\n",
      "Epoch  4,  Training Loss:    1.3128, Training Accuracy:     23.74,  Valid Accuracy:   22.49\n",
      "Epoch  5,  Training Loss:    1.2630, Training Accuracy:     24.77,  Valid Accuracy:   22.86\n",
      "Elapsed Time :  121.44\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = 9925\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "learning_rate = 1e-3\n",
    "epoch = 5\n",
    "num_layers = 1\n",
    "\n",
    "# Instantiate the model\n",
    "next_word_rnn = NextWordRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,num_layers=num_layers)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(next_word_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_next_word_rnn, train_acc_log_next_word_rnn, test_acc_log_next_word_rnn = train_model(next_word_rnn, loss_func, epoch, optimizer, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Vts31TwEw0jj"
   },
   "outputs": [],
   "source": [
    "accuracy, test_loss, perplexity= calculate_metrics(testloader, next_word_rnn,loss_func=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u938KCwVUqQ3"
   },
   "source": [
    "### LSTM with single hiddden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rWkdvZXqyhjI"
   },
   "outputs": [],
   "source": [
    "# Redefine the training function and accuracy calculator\n",
    "\n",
    "# Function to calculate accuracy for non-padded sequence\n",
    "def calculate_accuracy_LSTM(loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for data, l, target in loader:\n",
    "            # check if cuda is available\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            for i in range(len(l)):\n",
    "              output = model(data[i])  # Forward pass\n",
    "              _, predicted = torch.max(output[:l[i],:].data, 1)  # Get the predicted classes\n",
    "              total += target[i][:l[i]].view(-1).size(0)\n",
    "              correct += (predicted == target[i][:l[i]].view(-1)).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "def train_model(model, loss_func, num_epochs, optimizer, train_loader, test_loader):\n",
    "\n",
    "  train_loss_log = []\n",
    "  train_acc_log = []\n",
    "  test_acc_log = []\n",
    "\n",
    "  # Move model to GPU if CUDA is available\n",
    "  if torch.cuda.is_available():\n",
    "      model = model.cuda()\n",
    "  tic = time.time()\n",
    "  for epoch in range(1,num_epochs+1):\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "      x, l, y = data\n",
    "      # check if cuda is available\n",
    "      if torch.cuda.is_available():\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "      # get predicted y value from our current model\n",
    "      pred_y = model(x)\n",
    "      # calculate the loss\n",
    "      loss = loss_func(pred_y.view(-1,9925),y.view(-1))\n",
    "      # Zero the gradient of the optimizer\n",
    "      optimizer.zero_grad()\n",
    "      # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "      loss.backward()\n",
    "      # update weights ar\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item() * len(x)\n",
    "    # change the model to evaluation mode to calculate the test loss; We will come back to this later after learning Dropout and Batch Normalization\n",
    "    train_loss/=len(train_loader)\n",
    "    train_loss_log.append(train_loss)\n",
    "    train_acc = calculate_accuracy_LSTM(train_loader, model)\n",
    "    test_acc = calculate_accuracy_LSTM(test_loader, model)\n",
    "    train_acc_log.append(train_acc)\n",
    "    test_acc_log.append(test_acc)\n",
    "    print(\"Epoch {:2},  Training Loss: {:9.4f}, Training Accuracy: {:9.2f},  Test Accuracy: {:7.2f}\".format(epoch, train_loss, train_acc, test_acc))\n",
    "  toc = time.time()\n",
    "  print(\"Elapsed Time : {:7.2f}\".format(toc-tic))\n",
    "  return train_loss_log, train_acc_log, test_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4Ekh7FeWLKNl"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class NextWordLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(NextWordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        all_h, (h, c) = self.lstm(x)\n",
    "        out = self.fc(all_h) # Apply Linear layer to outputs from all the hidden state.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6QwSV8QLMKC",
    "outputId": "166ba06e-b487-40c6-e444-ebfae394bfcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1,  Training Loss:   60.7425, Training Accuracy:     16.63,  Test Accuracy:   16.69\n",
      "Epoch  2,  Training Loss:   49.0273, Training Accuracy:     19.98,  Test Accuracy:   19.82\n",
      "Epoch  3,  Training Loss:   46.2464, Training Accuracy:     21.83,  Test Accuracy:   21.41\n",
      "Epoch  4,  Training Loss:   44.4125, Training Accuracy:     23.16,  Test Accuracy:   22.32\n",
      "Epoch  5,  Training Loss:   43.0051, Training Accuracy:     24.10,  Test Accuracy:   22.96\n",
      "Elapsed Time :  130.24\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = 9925\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "learning_rate = 1e-3\n",
    "epoch = 5\n",
    "num_layers = 1\n",
    "\n",
    "# Instantiate the model\n",
    "next_word_lstm = NextWordLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,num_layers=num_layers)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(next_word_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_next_word_lstm, train_acc_log_next_word_lstm, test_acc_log_next_word_lstm = train_model(next_word_lstm, loss_func, epoch, optimizer, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khzGFBUzLcsT",
    "outputId": "15e24a7b-240e-4f31-a035-a8349c451e37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.960116448326055"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy_LSTM(testloader, next_word_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UodI7dB_yiHf"
   },
   "source": [
    "## Question 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Teswr5sNUqQ8"
   },
   "source": [
    "### RNN/LSTM with Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BghKyd0Ayjf_",
    "outputId": "6c3ce8b4-47f2-4e91-a04f-50be61d60308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1,  Training Loss:    1.9537, Training Accuracy:     15.23,  Valid Accuracy:   15.35\n",
      "Epoch  2,  Training Loss:    1.5786, Training Accuracy:     19.22,  Valid Accuracy:   19.04\n",
      "Epoch  3,  Training Loss:    1.4803, Training Accuracy:     21.34,  Valid Accuracy:   20.82\n",
      "Epoch  4,  Training Loss:    1.4177, Training Accuracy:     22.79,  Valid Accuracy:   21.92\n",
      "Epoch  5,  Training Loss:    1.3713, Training Accuracy:     24.04,  Valid Accuracy:   22.67\n",
      "Elapsed Time :  177.66\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NextWordLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):  # Updated num_layers to 2 hidden layers\n",
    "        super(NextWordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        all_h, (h, c) = self.lstm(x)\n",
    "        out = self.fc(all_h)  # Apply Linear layer to outputs from all the hidden states.\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 9925  # 10000 words in the dictionary + <START>, <UNK>, <UNUSED>\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 5\n",
    "NUM_LAYERS = 2  # Updated this to reflect the change in the model constructor\n",
    "\n",
    "# Instantiate the model\n",
    "next_word_lstm2 = NextWordLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Correct instantiation of the optimizer for the 2HL model\n",
    "optimizer = optim.Adam(next_word_lstm2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Assuming trainloader and testloader are properly defined and available\n",
    "train_loss_next_word_lstm, train_acc_log_next_word_lstm, test_acc_log_next_word_lstm = train_model(next_word_lstm2, loss_func, EPOCHS, optimizer, trainloader, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xi63cwR3L1Mv"
   },
   "outputs": [],
   "source": [
    "calculate_accuracy_LSTM(testloader, next_word_lstm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbGcXjF3PW7r"
   },
   "source": [
    "## Question 6. Generate Text with Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VAx7u3lMxWYw"
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt,model,max_length=10):\n",
    "  text = process_raw_text(prompt,vocab)[0][:-1] # Remove the last <EOS> token from our text\n",
    "  if torch.cuda.is_available():\n",
    "    text = text.cuda()\n",
    "  max_length = 10\n",
    "  for i in range(max_length):\n",
    "    # get the next word prediction and add it to text\n",
    "    output = model(text).view(-1,9925)\n",
    "    _, predicted = torch.max(output,1)\n",
    "    new_word = predicted[-1].unsqueeze(0)\n",
    "    text = torch.cat((text,new_word),dim=0)\n",
    "    text = text.to(torch.long)\n",
    "    if vocab.lookup_tokens([new_word.item()])[0] == '<EOS>':\n",
    "      break\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_pzqKcdYaXYU",
    "outputId": "b7ddd12c-a6b5-49cc-f0bb-fad6568431d1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<SOS> the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt to tokenized tensor\n",
    "prompt=[''] # You do not need to prepend <SOS> token to your prompt. generate_text() function will prepend <SOS> token for you.\n",
    "generated_text = generate_text(prompt,next_word_lstm2) # put your model here.\n",
    "generated_text_lookup = vocab.lookup_tokens(generated_text.tolist())\n",
    "' '.join(generated_text_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jyOrX8dOYv-E",
    "outputId": "8840194b-2043-4b78-9c29-f3fe9915d984"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<SOS> some traders are <unk> by the <unk> of the <unk> of the'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=['Some Traders'] # You do not need to prepend <SOS> token to your prompt. generate_text() function will prepend <SOS> token for you.\n",
    "generated_text = generate_text(prompt,next_word_lstm2) # put your model here.\n",
    "generated_text_lookup = vocab.lookup_tokens(generated_text.tolist())\n",
    "' '.join(generated_text_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NUylqf4KfYih",
    "outputId": "ba0d5c1e-dfd3-45d1-87a9-f5677a115e9c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<SOS> stocks the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=['stocks'] # You do not need to prepend <SOS> token to your prompt. generate_text() function will prepend <SOS> token for you.\n",
    "generated_text = generate_text(prompt,next_word_lstm2) # put your model here.\n",
    "generated_text_lookup = vocab.lookup_tokens(generated_text.tolist())\n",
    "' '.join(generated_text_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XATffDwUflxM",
    "outputId": "a2cc29c9-7a12-45f4-8973-1ac4a54adf9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic Regression Model Parameters: 1994925\n",
      "Single Layer Hidden Network Model Parameters: 2285753\n",
      "Next Word RNN Model Parameters: 2583749\n",
      "Next Word LSTM Model Parameters: 2682821\n",
      "Next Word LSTM with 2 layers Model Parameters: 2814917\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count parameters for each model\n",
    "num_params_multinomial_logistic_regression = count_parameters(model_a)\n",
    "num_params_single_layer_hidden_network = count_parameters(model_b)\n",
    "num_params_next_word_rnn = count_parameters(next_word_rnn)\n",
    "num_params_next_word_lstm = count_parameters(next_word_lstm)\n",
    "num_params_next_word_lstm2 = count_parameters(next_word_lstm2)\n",
    "\n",
    "print(f\"Multinomial Logistic Regression Model Parameters: {num_params_multinomial_logistic_regression}\")\n",
    "print(f\"Single Layer Hidden Network Model Parameters: {num_params_single_layer_hidden_network}\")\n",
    "print(f\"Next Word RNN Model Parameters: {num_params_next_word_rnn}\")\n",
    "print(f\"Next Word LSTM Model Parameters: {num_params_next_word_lstm}\")\n",
    "print(f\"Next Word LSTM with 2 layers Model Parameters: {num_params_next_word_lstm2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uTHJmEQGUwD"
   },
   "source": [
    "Examining the performance of all the models:\n",
    "\n",
    "\n",
    "Multinomial Logistic Regression:\n",
    "Training accuracy reached a maximum of 20.32%, and the validation Accuracy reached up to 18.68%. This basic model is not as computationally demanding but also cannot capture complex patterns.\n",
    "\n",
    "Unigram Model: 19.32% test accuracy\n",
    "Bigram Model: 19.37% test accuracy\n",
    "\n",
    "Single Layer Neural Network:\n",
    "Training accuracy reached a maximum of 19.23%, and the validation Accuracy reached up to 18.97%.\n",
    "The nonlinearity in the activation function allows it to capture more complex patterns.\n",
    "\n",
    "RNN with single hidden layer Model:\n",
    "Training accuracy reached a maximum of 24.77%, and the validation Accuracy reached up to 22.86%.\n",
    "\n",
    "LSTM with single hidden layer model:\n",
    "Training accuracy reached a maximum of 24.10%, and the validation Accuracy reached up to 22.96%. Advantage of the LSTM is ability to capture dependencies and relationship among words\n",
    "\n",
    "LSTM with 2 hidden layers model:\n",
    "Training accuracy reached a maximum of 24.04%, and the validation Accuracy reached up to 22.67%. Similar benefits as above with added ability to capture more complex patterns with 2 hidden layers.\n",
    "\n",
    "After examining the next word accuracy on each model and taking into account factors like computational costs, interpretability, and run time, I chose to go with the LSTM neural net that used 2 hidden layers.\n",
    "Since we were tasked with predicting the next word for the Penn Treebank dataset, which has many complex patterns and dependencies in the text data,I I concluded that an LSTM model was best suited because it can capture long-term dependencies in sequential data.\n",
    "Within the two LSTM models, I decided to g with the two hidden layer LSTM even thouhg the single layer one had a similar valiadation accuracy because the model with 2 hidde layers will be able to capture more complex patterns because of the additional layer and could have had better performance given more tests/ training epochs.\n",
    "Among the LSTM models, both the single hidden layer LSTM and the two hidden layers LSTM performed similarly in terms of validation accuracy. However, the two hidden layers LSTM might have slightly more capacity to capture complex patterns due to its additional hidden layer.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
